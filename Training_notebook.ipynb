{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgvO19IKHCxR"
      },
      "outputs": [],
      "source": [
        "# prompt: import all needed for efficientnetv2-s model training\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: custom train efficientnetv2-s using TPU, 20 epochs,  dataset name=audi_dataset.zip\n",
        "\n",
        "import numpy as np\n",
        "# Check if TPU is available\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  print('Running on TPU')\n",
        "except ValueError:\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "  print('Running on CPU/GPU')\n",
        "\n",
        "\n",
        "# Download and unzip the dataset\n",
        "!wget <URL_TO_YOUR_DATASET> -O audi_dataset.zip\n",
        "!unzip audi_dataset.zip\n",
        "\n",
        "# Define data loading and preprocessing functions\n",
        "def preprocess_image(image_path):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.image.resize(image, [224, 224])\n",
        "  image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
        "  return image\n",
        "\n",
        "\n",
        "def load_dataset(image_paths, labels):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "  dataset = dataset.map(lambda image_path, label: (preprocess_image(image_path), label),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Assuming your dataset is structured with folders for each class\n",
        "# and image files inside each folder\n",
        "import pathlib\n",
        "data_dir = pathlib.Path('audi_dataset')\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print('Total images:', image_count)\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "print('Classes:', CLASS_NAMES)\n",
        "\n",
        "\n",
        "# Create training and validation datasets\n",
        "# ... (Implement your own data splitting logic, you can use sklearn.model_selection.train_test_split)\n",
        "\n",
        "# Create the model with TPU strategy\n",
        "with strategy.scope():\n",
        "  # Load EfficientNetV2-S from TensorFlow Hub\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_s/feature_vector/2\",\n",
        "                     trainable=True),\n",
        "      layers.Dense(len(CLASS_NAMES), activation='softmax')\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=20, validation_data=validation_dataset)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_dataset)\n",
        "print('Validation accuracy:', accuracy)\n",
        "\n",
        "# Save the model\n",
        "model.save('efficientnetv2_s_audi_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "mroOIBVlHTi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert model as tflite\n",
        "\n",
        "# Convert the Keras model to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open('efficientnetv2_s_audi_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "oy1Sc8LgHjOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}